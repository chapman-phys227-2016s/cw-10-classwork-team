{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classwork 10\n",
    "### Michael Seaman, Taylor Patti, Austin Ayers, Chinmai Raman, Andrew Malfavon\n",
    "#### 4/26/16\n",
    "\n",
    "In summary, the following integration methods are all largely similar - they take an initial point and analyze its integral by taking tiny steps in one direction and guessing the values of the function based on the output and it's derivative.\n",
    "\n",
    "1. Euler's Method: \n",
    "   \n",
    "   $u_{k+1} = u_k + \\Delta t\\, f[t_k, u_k]$ \n",
    "   \n",
    "   Euler's Method is what I call the naive integration method. It guesses the output of the next point on the integral based on the derivative at the point we're evaluating. We then add the product of that derivative and the timestep to the given condition to get the next point.\n",
    "   \n",
    "   \n",
    "1. Leapfrog (Midpoint) Method: \n",
    "   \n",
    "   $u_{k+1} = u_{k-1} + 2\\Delta t\\, f[t_k, u_k]$  \n",
    "   \n",
    "   The Leapfrog method makes a small improvement over the Euler method by using the the point previous to the one we're focusing on to evaluate the next one. We use, however the derivative at the point that we're focusing on. In total, we end up with a pattern of using the previous point, adding the product of 2 stepsizes and the derivative at the point to hop over and get the next point. Because of this we actually need 2 initial points - but we can cheat this by guessing that second point with Euler's.\n",
    "   \n",
    "   \n",
    "1. Heun's (Trapezoid) Method: \n",
    "   \n",
    "   $\\tilde{u}_{k+1} = u_k + \\Delta t\\, f[t_k, u_k]$, \n",
    "   \n",
    "   $u_{k+1} = u_k + (\\Delta t/2)(f[t_k, u_k] + f[t_{k+1}, \\tilde{u}_{k+1}])$  \n",
    "   \n",
    "   The traaaapaziod method makes a significant jump in performance from the Leapfrog method when evaluating it on the function we did in class: $ f(t) = e^{\\frac{1}{2}t^2} $. Instead of evaluating the derivative at a single point, we instead calculate it at two points: $ u_k $ and $ u_{k+1} $, subsequently taking the average of the two. This can be seen in the second part of the  u function. The two points it averages are the point given, and the point given by $\\tilde{u}$ - which is Euler's method. \n",
    "   \n",
    "1. 2nd-order Runge-Kutta Method: \n",
    "   \n",
    "   $u_{k+1} = u_k + K_1$, \n",
    "   \n",
    "   $K_1 = \\Delta t\\, f[t_k, u_k]$, \n",
    "   \n",
    "   $K_2 = \\Delta t\\, f[t_k + \\Delta t/2, u_k + K_1/2]$  \n",
    "   \n",
    "   The 2nd-order RK is almost exactly the Heun's method with one key difference.  While both methods evaluate an preliminary step of the Euler method, they treat the result differently. In Heun's method we take the aveage of the given and found point. However, in RK-2, we instead use that found point to guess what the function's value is at t + one-half a stepsize ( $t + \\frac{\\Delta t}{2} $). We then evaluate the slope at that point, and use the found slope to evaluate the function's value at the next step.\n",
    "   \n",
    "   \n",
    "1. 4th-order Runge-Kutta Method: \n",
    "   \n",
    "   $u_{k+1} = u_k + (K_1 + 2K_2 + 2K_3 + K_4)/6$, \n",
    "   \n",
    "   $K_1 = \\Delta t\\,f[t_k,u_k]$, \n",
    "   \n",
    "   $K_2 = \\Delta t\\, f[t_k + \\Delta t/2, u_k + K_1/2]$, \n",
    "   \n",
    "   $K_3 = \\Delta t\\, f[t_k + \\Delta t/2, u_k + K_2/2]$, \n",
    "   \n",
    "   $K_4 = \\Delta t\\,f[t_k + \\Delta t, u_k + K_3]$  \n",
    "   \n",
    "   The RK-4 method essentially builds off of the RK-2 method, as, in principle, all of the inner workings are the same (in fact, we can see the RK-2 method as the first two steps of RK-4). However, we can squeeze some extra accuracy by repeating the the second step more than once. This repeatedely using our guessed point to find slopes in between can get us closer to the actual function by 2 orders of magnitude faster than the other methods. The final step can be considered a weighted average of the computed values, with more \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (SageMath)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
